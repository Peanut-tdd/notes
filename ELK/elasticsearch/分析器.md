# elasticsearch分析器

**分析器用来做什么**

*分析* 包含下面的过程：

- 首先，将一块文本分成适合于倒排索引的独立的 *词条* ，
- 之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 *recall*

分析器执行上面的工作。（通俗一点的说法：将文本域拆分成多个term词条；同时给term规定一个标准化格式，用以搜索）





**分析器组成**

- **Character filters （字符过滤器）**

  首先，字符串按顺序通过每个 *字符过滤器* 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 `&` 转化成 `and`。

  一个分析器可能有0个或多个字符过滤器，它们按顺序应用。

- **Tokenizer （分词器）**

  其次，字符串被 *分词器* 分为单个token（词条）。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条，输出一个token流。

  分词器还负责记录每个term的顺序或位置，以及该term所表示的原单词的开始和结束字符偏移量。

  一个分析器必须只能有一个分词器

- **Token filters （token过滤器）**

  token过滤器接收token流，并且可能会添加、删除或更改tokens。

  不允许token过滤器更改每个token的位置或字符偏移量。

  一个分析器可能有0个或多个token过滤器，它们按顺序应用。





**内置分析器**

词条实例：Set the shape to semi-transparent by calling set_trans(5)

- **标准分析器**

  它根据 [Unicode 联盟](http://www.unicode.org/reports/tr29/) 定义的 *单词边界* 划分文本。删除绝大部分标点。最后，将词条小写

  ```
  set, the, shape, to, semi, transparent, by, calling, set_trans, 5
  ```

- **简单分析器**

  简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生

  ```
  set, the, shape, to, semi, transparent, by, calling, set, trans
  ```

-  **空格分析器**

  空格分析器在空格的地方划分文本。它会产生

  ```
  Set, the, shape, to, semi-transparent, by, calling, set_trans(5)
  ```

- **语言分析器**

  特定语言分析器可用于 [很多语言](https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-lang-analyzer.html)。它们可以考虑指定语言的特点。例如， `英语` 分析器附带了一组英语无用词（常用单词，例如 `and` 或者 `the` ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 *词干* 。

  ```
  set, shape, semi, transpar, call, set_tran, 5
  ```



测试：

```
GET /_analyze
{
  "analyzer": "standard",
  "text": "Text to analyze"
}
```



**创建自定义分析器**

当内置的分词器无法满足需求时，可以创建`custom`类型的分词器。

- `tokenizer`:内置或定制的tokenizer.(必须)
- `char_filter`:内置或定制的char_filter(非必须)
- `filter`:内置或定制的token filter(非必须)

```
PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer":{
          "type":"custom",
          "tokenizer":"standard",
          "char_filter":["html_strip"],
          "filter":["lowercase","asciifolding"]
        }
      }
    }
  }
}
```

上面的示例中定义了一个名为`my_custom_analyzer`的分词器，该分词器的`type`为`custom`，`tokenizer`为`standard`，`char_filter`为`hmtl_strip`,`filter`定义了两个分别为：`lowercase`和`asciifolding`。运行分词测试：



```bash
POST my_index/_analyze
{
  "analyzer": "my_custom_analyzer",
   "text": "Is this <b>déjà vu</b>?"
}
```



---





**什么时候用分析器**

使用地方有两个：

- 创建索引时（如下）
- 进行搜索时



**创建索引分析器使用**

- **索引指定默认分析器**

  ```
  PUT lol_team
  {
    "settings": {
      "analysis": {
        "analyzer": {
          "default":{
            "type":"simple"
          },
            "default_search":{             #指定搜索分析器default_search
            "type":"whitespace"
          }
        }
      }
    }
  }
  
  
  GET lol_team/_analyze
  {
  	"text":"Set the shape to semi-transparent by calling set_trans(5)"
  }
  
  ```

- **为字段指定分析器**

  ```
  DELETE lol_team 
  
  PUT lol_team
  {
    "settings": {
      "number_of_shards": 1
    }, 
    "mappings": {
      "china":{
        "properties":{
          "name":{
            "type":"text",
            "analyzer":"whitespace",
            "search_analyzer":"simple"                  #指定搜索分析器
          }
        }
      }
    }
  }
  
  
  
  GET lol_team/_analyze
  {
    "field": "name", 
    "text": "Set the shape to semi-transparent by calling set_trans(5)"
  }
```
  

  

  

  




**搜索时如何确定分析器**

在搜索时，通过下面参数依次检查搜索时使用的分词器：

- 搜索时指定`analyzer`参数      

  ```
  GET my_index/_search
  {
    "query": {
      "match": {
        "message": {
          "query": "Quick foxes",
          "analyzer": "stop"
        }
      }
    }
  }
  ```

  

- 创建mapping时指定字段的`search_analyzer`属性（如上：为字段指定分析器）
- 创建索引时指定`setting`的`analysis.analyzer.default_search`（如上：索引指定默认分析器）
- 查看创建索引时字段指定的`analyzer`属性













**小结&回顾**

- analyzer（分析器）是一个包，这个包由三部分组成，分别是：character filters （字符过滤器）、tokenizer（分词器）、token filters（token过滤器）
- 一个analyzer可以有0个或多个character filters
- 一个analyzer有且只能有一个tokenizer
- 一个analyzer可以有0个或多个token filters
- character filter 是做字符转换的，它接收的是文本字符流，输出也是字符流
- tokenizer 是做分词的，它接收字符流，输出token流（文本拆分后变成一个一个单词，这些单词叫token）
- token filter 是做token过滤的，它接收token流，输出也是token流
- 由此可见，整个analyzer要做的事情就是将文本拆分成单个单词，文本 ----> 字符 ----> token



![](https://raw.githubusercontent.com/Peanut-tdd/Picture/main/analyzer.jpg)

